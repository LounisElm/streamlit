{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from surprise.model_selection import train_test_split, KFold, cross_validate, LeaveOneOut\n",
    "from surprise import Dataset, SVD, Reader, accuracy\n",
    "from loaders import load_ratings\n",
    "from loaders import load_items\n",
    "from configs import EvalConfig\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c35f9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dataset = load_ratings(surprise_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab56bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate predictions on a random test set specified in eval_config\"\"\"\n",
    "    # -- implement the function generate_split_predictions --\n",
    "    trainset, testset = train_test_split(ratings_dataset, test_size=EvalConfig.test_size)\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    # -- implement the function generate_loo_top_n --\n",
    "    loo = LeaveOneOut(n_splits=1, random_state=1)\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    algo.fit(trainset)\n",
    "    anti_test_set = trainset.build_anti_testset()\n",
    "    predictions = algo.test(anti_test_set)\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    \n",
    "\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # -- implement the function generate_full_top_n --\n",
    "    full_train_set = ratings_dataset.build_full_trainset()\n",
    "    algo.fit(full_train_set)\n",
    "    anti_test_set = full_train_set.build_anti_testset()\n",
    "    predictions = algo.test(anti_test_set)\n",
    "    anti_testset_top_n = get_top_n(predictions, n=eval_config.top_n_value)\n",
    "    return anti_testset_top_n\n",
    "\n",
    "\n",
    "def precompute_information():\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to popularity rankings\n",
    "    \"\"\"\n",
    "    precomputed_dict = {}\n",
    "    df_ratings = load_ratings()\n",
    "\n",
    "    popularity_counts = df_ratings[C.ITEM_ID_COL].value_counts()\n",
    "    item_to_rank = {item_id: rank+1 for rank, item_id in enumerate(popularity_counts.index)}\n",
    "    precomputed_dict[\"item_to_rank\"] = item_to_rank\n",
    "\n",
    "    return precomputed_dict       \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T\n",
    "\n",
    "    info = precompute_information()\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"Calcule le taux moyen de succès (hit rate) pour chaque utilisateur.\n",
    "\n",
    "    Un \"hit\" signifie que le film omis du testset figure parmi les recommandations du top-n.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    # Parcours de chaque observation dans le testset\n",
    "    for user, left_out_movie, _ in testset:\n",
    "        # Recommandations top-n pour l'utilisateur courant\n",
    "        user_top_n = anti_testset_top_n.get(user, [])\n",
    "\n",
    "        # Liste des ID des films recommandés\n",
    "        recommended_movies = [movie_id for movie_id, _ in user_top_n]\n",
    "\n",
    "        # Incrémentation du nombre de hits si le film est dans les recommandations\n",
    "        if left_out_movie in recommended_movies:\n",
    "            hits += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    # Calcul final du taux de hit\n",
    "    return hits / total if total else 0\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "        item_to_rank (dict): {item_id: popularity_rank}\n",
    "    \"\"\"\n",
    "\n",
    "    total_novelty = 0\n",
    "    total_users = len(anti_testset_top_n)\n",
    "\n",
    "    for user_id, recommendations in anti_testset_top_n.items():\n",
    "        user_novelty = 0\n",
    "        for item_id, _ in recommendations:\n",
    "            rank = item_to_rank.get(item_id, 0)  # 0 or a high number if unknown\n",
    "            user_novelty += rank\n",
    "        total_novelty += user_novelty\n",
    "\n",
    "    avg_novelty = total_novelty / total_users if total_users > 0 else 0\n",
    "    return avg_novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model LinReg\n",
      "Training split predictions\n",
      "- computing metric MAE\n",
      "- computing metric RMSE\n",
      "Training loo predictions\n",
      "Training full predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hit_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "novelty",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6e5f3af4-b8e3-49f2-9e2a-d1fc4b40c386",
       "rows": [
        [
         "LinReg",
         "0.7608995672348252",
         "0.9762915309598977",
         "0.0014903129657228018",
         "205708.45752608048"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>novelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinReg</th>\n",
       "      <td>0.7609</td>\n",
       "      <td>0.976292</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>205708.457526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MAE      RMSE  hit_rate        novelty\n",
       "LinReg  0.7609  0.976292   0.00149  205708.457526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation report exported to: \\Users\\nicol\\Documents\\GitHub\\Majeur-BA\\RECOMMENDER-SYSTEM\\mlsmm2156\\evaluation\\2025_05_26_21_14_39_report.csv\n"
     ]
    }
   ],
   "source": [
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"MAE\": (accuracy.mae, {'verbose': False}),\n",
    "        # -- add new split metrics here --\n",
    "    \"RMSE\" : (accuracy.rmse, {'verbose': False}),\n",
    "    }\n",
    "    # -- add new types of metrics here --\n",
    "    ,\"loo\" : {\"hit_rate\" : (get_hit_rate, {})},\n",
    "    \"full\" : {\"novelty\" : (get_novelty, {})}\n",
    "}\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "algo = SVD()\n",
    "test = generate_split_predictions(algo, sp_ratings, EvalConfig)\n",
    "\n",
    "top_n_loo_top,test_set_loo = generate_loo_top_n(algo, sp_ratings, EvalConfig)\n",
    "rows = []\n",
    "for user_id, item_list in top_n_loo_top.items():\n",
    "    for item_id, estimated_rating in item_list:\n",
    "        rows.append((user_id, item_id, estimated_rating))\n",
    "\n",
    "df_topn = pd.DataFrame(rows, columns=['user', 'item', 'estimated_rating'])\n",
    "df_topn.to_csv(\"top_n_loo.csv\", index=False)\n",
    "\n",
    "top_n_full = generate_full_top_n(algo, sp_ratings, EvalConfig) \n",
    "rows = []\n",
    "for user_id, item_list in top_n_full.items():\n",
    "    for item_id, estimated_rating in item_list:\n",
    "        rows.append((user_id, item_id, estimated_rating))\n",
    "df_topn_full = pd.DataFrame(rows, columns=['user', 'item', 'estimated_rating'])\n",
    "\n",
    "df_topn_full.to_csv(\"top_n_full.csv\", index=False)\n",
    "precomputed_dict = precompute_information()\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "display(evaluation_report)\n",
    "export_evaluation_report(evaluation_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd0f52",
   "metadata": {},
   "source": [
    "\n",
    "- Erreurs MAE et RMSE élevées → faible précision.\n",
    "- hit_rate très bas (≈0.3 %) → recommandations peu pertinentes.\n",
    "- Valeur de novelty anormalement haute → Elle peut indiquer un problème de mise à l’échelle ou de logique dans la fonction de calcul : il est possible que le modèle recommande quasi exclusivement des films non populaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84354dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Feature method all_features not yet implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rmse\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m rmse = \u001b[43mevaluate_single_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCB_AllFeatures_RF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mContentBased\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeatures_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregressor_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom_forest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mevaluate_single_model\u001b[39m\u001b[34m(model_name, model_class, model_params, test_size)\u001b[39m\n\u001b[32m     20\u001b[39m trainset, testset = train_test_split(data, test_size=test_size, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Initialiser et entraîner le modèle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model = \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m model.fit(trainset)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Faire les prédictions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\Documents\\GitHub\\Majeur-BA\\RECOMMENDER-SYSTEM\\mlsmm2156\\models.py:844\u001b[39m, in \u001b[36mContentBased.__init__\u001b[39m\u001b[34m(self, features_method, regressor_method)\u001b[39m\n\u001b[32m    842\u001b[39m AlgoBase.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    843\u001b[39m \u001b[38;5;28mself\u001b[39m.regressor_method = regressor_method\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28mself\u001b[39m.content_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_content_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\Documents\\GitHub\\Majeur-BA\\RECOMMENDER-SYSTEM\\mlsmm2156\\models.py:854\u001b[39m, in \u001b[36mContentBased.create_content_features\u001b[39m\u001b[34m(self, features_method)\u001b[39m\n\u001b[32m    852\u001b[39m     df_features = df_items[C.LABEL_COL].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)).to_frame(\u001b[33m'\u001b[39m\u001b[33mn_character_title\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# (implement other feature creations here)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFeature method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not yet implemented\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_features\n",
      "\u001b[31mNotImplementedError\u001b[39m: Feature method all_features not yet implemented"
     ]
    }
   ],
   "source": [
    "def evaluate_single_model(model_name, model_class, model_params, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Évalue un seul modèle et retourne le RMSE\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Nom du modèle pour l'affichage\n",
    "        model_class: Classe du modèle à évaluer\n",
    "        model_params (dict): Paramètres du modèle\n",
    "        test_size (float): Proportion des données à utiliser pour le test\n",
    "    \n",
    "    Returns:\n",
    "        float: RMSE du modèle\n",
    "    \"\"\"\n",
    "    # Charger les données\n",
    "    df_ratings = load_ratings()\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    \n",
    "    # Split train-test\n",
    "    trainset, testset = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Initialiser et entraîner le modèle\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Faire les prédictions\n",
    "    predictions = model.test(testset)\n",
    "    \n",
    "    # Calculer le RMSE\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    \n",
    "    print(f\"Modèle: {model_name}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "\n",
    "    # Exemple d'utilisation\n",
    "rmse = evaluate_single_model(\n",
    "    \"CB_AllFeatures_RF\",\n",
    "    ContentBased,\n",
    "    {\"features_method\": \"all_features\", \"regressor_method\": \"random_forest\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
